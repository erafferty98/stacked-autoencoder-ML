{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AMLCWK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMh5rSHzgjqaoVB+jKfdIxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erafferty98/refactored-train/blob/master/Copy_of_AMLCWK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RdeX2l3jUb1"
      },
      "source": [
        "Process = create features with AE, select features with RFE + Extratrees, train adaboost classifier for predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN_F88BQ83JN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7086c055-bc3f-4c37-ffef-af9711e01d68"
      },
      "source": [
        "#Load libraries and data\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "import pandas as pd\n",
        "from pandas import set_option\n",
        "from numpy import unique\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from __future__ import print_function\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.regularizers import l1\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')\n",
        "set_seed(1)\n",
        "# load csv using pandas\n",
        "filename = 'test_imperson_without4n7_balanced_data.csv'\n",
        "testdata = read_csv(filename)\n",
        "filename = 'train_imperson_without4n7_balanced_data.csv'\n",
        "traindata = read_csv(filename)\n",
        "\n",
        "\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "#investigate data\n",
        "print(\"training data shape\",traindata.shape)\n",
        "print(\"testing data shape\",testdata.shape)\n",
        "description = traindata.describe()\n",
        "print(description)\n",
        "class_counts = traindata.groupby('155').size()\n",
        "print(class_counts)\n",
        "traindata.head()\n",
        "\n",
        "#train_set, test_set = train_test_split(traindata, test_size=testdata, random_state=42)\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "x_train = traindata[columns]\n",
        "y_train = traindata[target]\n",
        "\n",
        "x_test = testdata[columns]\n",
        "y_test = testdata[target]\n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)\n",
        "good = traindata[y_train == 1]\n",
        "bad = traindata[y_train == 0]\n",
        "print(good.shape,bad.shape)\n",
        "\n",
        "normal_duplicates = sum( good.duplicated() )\n",
        "attack_duplicates = sum( bad.duplicated() )\n",
        "total_duplicates = normal_duplicates + attack_duplicates\n",
        "\n",
        "print( 'Normal duplicates', normal_duplicates )\n",
        "print( 'Attack duplicates', attack_duplicates )\n",
        "print( 'Total duplicates', total_duplicates )\n",
        "print( 'Fraction duplicated', total_duplicates / len(traindata) )\n",
        "\n",
        "df_nUnique = traindata.loc[:, traindata.nunique() == 1]\n",
        "df_nUnique.head()\n",
        "\n",
        "def Drop_Redun_Columns(df):\n",
        "  print (df.shape , 'Traning Data Shape Before drop')\n",
        "  dfdrop = df.loc[:, df.nunique() != 1]\n",
        "  print (dfdrop.shape , 'Traning Data Shape After drop')\n",
        "  return dfdrop\n",
        "\n",
        "# originally deleted non unique columns but decided to keep. Left code in to see how many are non-unique and how these correlate\n",
        "unique_training = Drop_Redun_Columns(traindata)\n",
        "print (\"\\n\")\n",
        "\n",
        "%matplotlib inline\n",
        "corrl = unique_training.corr()\n",
        "plt.imshow(corrl) ;\n",
        "\n",
        "#stacked autoencoder sparse. used by demise and detered\n",
        "# split into train test sets\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "start_time = time.time()\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)\n",
        "\n",
        "input_size = 152\n",
        "hidden_size = 100\n",
        "hidden_size_2 = 75\n",
        "hidden_size_3 = 50\n",
        "code_size = 20\n",
        "batch_size = 5\n",
        "input_data = Input(shape=(input_size,))\n",
        "encoder_1 = Dense(hidden_size, activation='relu')(input_data)\n",
        "encoder_2 = Dense(hidden_size_2, activation='relu')(encoder_1)\n",
        "encoder_3 = Dense(hidden_size_3, activation='relu')(encoder_2)\n",
        "code = Dense(code_size, activation='relu')(encoder_3)\n",
        "decoder_1 = Dense(hidden_size, activation='relu')(code)\n",
        "output_data = Dense(input_size, activation='sigmoid')(decoder_1)\n",
        "autoencoder = Model(input_data, output_data) # autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder_train = autoencoder.fit(X_train, X_train, epochs=8,validation_data=(X_test, X_test),batch_size=batch_size)\n",
        "autoencoder.summary()\n",
        "encoder = Model(input_data, code) # encoder to create new features\n",
        "\n",
        "# encode the train data\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "# encode the test data\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "# define the model\n",
        "model = LogisticRegression() #use basic logisitic regression to test the autoencoder results\n",
        "# fit the model on the encoded training set and regular training set\n",
        "model.fit(X_train_encode, Y_train)\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_encode)\n",
        "# calculate classification accuracy\n",
        "acc = accuracy_score(Y_test, yhat)\n",
        "# now test with original dataset\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "yhat2 = model.predict(x_test)\n",
        "acc2 = accuracy_score(y_test, yhat2)\n",
        "print(\"autodencoder lr: \", acc, \"\\nbase data lr: \",acc2)\n",
        "\n",
        "# trained the autoencoder and checked the accuracy. Now to encode whole dataset.\n",
        "# encode the train data\n",
        "x_train_en = encoder.predict(x_train)\n",
        "# encode the test data\n",
        "x_test_en = encoder.predict(x_test)\n",
        "\n",
        "\n",
        "# sorting out testing/trainng sets\n",
        "print(type(x_train),type(x_train_en),type(y_train))\n",
        "x_train_en = pd.DataFrame(x_train_en)\n",
        "traindata = pd.concat([x_train,x_train_en,y_train],axis=1,ignore_index=True)\n",
        "\n",
        "x_test_en = pd.DataFrame(x_test_en)\n",
        "testdata = pd.concat([x_test,x_test_en,y_test],axis=1,ignore_index=True)\n",
        "\n",
        "print(traindata.shape)\n",
        "print(traindata.head())\n",
        "print(testdata.shape)\n",
        "print(testdata.head())\n",
        "\n",
        "#split into x and y for feature selection\n",
        "x_train = traindata[traindata.columns[0:(input_size + code_size)]]\n",
        "y_train = traindata[traindata.columns[-1]]\n",
        "\n",
        "x_test = testdata[testdata.columns[0:(input_size + code_size)]]\n",
        "y_test = testdata[testdata.columns[-1]]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "%matplotlib inline\n",
        "# plot losses\n",
        "plt.close('all')\n",
        "plt.figure()\n",
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(1,9)\n",
        "plt.plot(epochs,loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs,val_loss, 'g-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# feature selection\n",
        "X = x_train\n",
        "Y = y_train\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "# feature extraction with RFE 3 features\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "model = LogisticRegression(solver='liblinear',random_state=42)\n",
        "# feature extraction\n",
        "rfe = RFE(model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "rfebestindices10 = [fit.get_support(indices=True)]\n",
        "print(\"Selected Features indices: %s\" % rfebestindices10)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
        "print(\"end RFE extraction\\n\")\n",
        "\n",
        "# feature extraction with extra trees classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# feature extraction\n",
        "extratrees = ExtraTreesClassifier(max_features=4,random_state=42)\n",
        "fit = extratrees.fit(X, Y)\n",
        "result = fit.feature_importances_\n",
        "\n",
        "important_features_dict = {}\n",
        "for x,i in enumerate(result):\n",
        "    important_features_dict[x]=i\n",
        "\n",
        "\n",
        "important_features_list = sorted(important_features_dict,\n",
        "                                 key=important_features_dict.get,\n",
        "                                 reverse=True)\n",
        "\n",
        "print( 'Most important features: %s' %important_features_list)\n",
        "\n",
        "\n",
        "rfebestindices10_unpacked = list(rfebestindices10[0])\n",
        "print(rfebestindices10_unpacked)\n",
        "print(important_features_list[:4])\n",
        "print(type(rfebestindices10_unpacked))\n",
        "features = set(rfebestindices10_unpacked + important_features_list[:4])\n",
        "features = list(features) \n",
        "features = sorted(features)\n",
        "print(features)\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, hamming_loss \n",
        "from tensorflow.random import set_seed\n",
        "from sklearn.metrics import plot_confusion_matrix,roc_curve, jaccard_score,average_precision_score,balanced_accuracy_score\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "set_seed(1)\n",
        "#selected model\n",
        "\n",
        "#building ML algorithms\n",
        "set_seed(1)\n",
        "#model creation\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "x_test = testdata.iloc[:,features]\n",
        "\n",
        "# evaluate model\n",
        "model = SVC(kernel='linear', gamma=0.1, random_state = 42,probability=True)\n",
        "kfold = KFold(n_splits=10, random_state=42,shuffle=True)\n",
        "cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
        "msg = \"Linear SVM: %f (%f)\" % (cv_results.mean(), cv_results.std())\n",
        "print(msg)\n",
        "\n",
        "%matplotlib inline\n",
        "#evaluating model and analyzing results\n",
        "set_seed(1)\n",
        "#selected model is classifier with logistic regression RFE + extra trees feature selection\n",
        "model = model.fit(X,Y)\n",
        "predictions = model.predict(x_test)\n",
        "#plot ROC AUC\n",
        "# predict probabilities\n",
        "probs = model.predict_proba(x_test)\n",
        "# keep probabilities for the attack outcome only\n",
        "probs = probs[:,1]\n",
        "# calculate AUC\n",
        "auc = roc_auc_score(y_test, probs)\n",
        "print('AUC: %.3f' % auc)\n",
        "# calculate roc curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "# plot no skill\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, predictions),classification_report(y_test, predictions))\n",
        "\n",
        "def sk_metrix(real_values,pred_values):\n",
        "   Accuracy = round( accuracy_score(real_values,pred_values) ,4)\n",
        "   \n",
        "   Precision  = round(precision_score(real_values,pred_values),4 )\n",
        "   \n",
        "   Recall     = round(recall_score(real_values,pred_values),4 )\n",
        "   F1         = round (f1_score(real_values,pred_values),4)\n",
        "   MCC        = round (matthews_corrcoef(real_values,pred_values)  ,4)\n",
        "   Hamming    = round (hamming_loss(real_values,pred_values) ,4)\n",
        "   Jaccard    = round (jaccard_score(real_values,pred_values) ,4)\n",
        "   Prec_Avg   = round (average_precision_score(real_values,pred_values) ,4)\n",
        "   Accu_Avg   = round (balanced_accuracy_score(real_values,pred_values) ,4)\n",
        "   mat_met = pd.DataFrame({\n",
        "'Metric': ['Accuracy','Precision','Recall','F1','MCC','Hamming','Jaccard'],\n",
        "'Value': [Accuracy,Precision,Recall,F1,MCC,Hamming,Jaccard]})\n",
        "   return (mat_met)\n",
        "\n",
        "sk_met = sk_metrix(y_test, predictions)\n",
        "print(sk_met)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "display.max_colwidth : int or None\n",
            "    The maximum width in characters of a column in the repr of\n",
            "    a pandas data structure. When the column overflows, a \"...\"\n",
            "    placeholder is embedded in the output. A 'None' value means unlimited.\n",
            "    [default: 50] [currently: 400]\n",
            "training data shape (97044, 153)\n",
            "testing data shape (40158, 153)\n",
            "             1        2        3  ...      153           154           155\n",
            "count  97044.0  97044.0  97044.0  ...  97044.0  97044.000000  97044.000000\n",
            "mean       0.0      0.0      0.0  ...      0.0      0.178474      0.500000\n",
            "std        0.0      0.0      0.0  ...      0.0      0.360078      0.500003\n",
            "min        0.0      0.0      0.0  ...      0.0      0.000000      0.000000\n",
            "25%        0.0      0.0      0.0  ...      0.0      0.000000      0.000000\n",
            "50%        0.0      0.0      0.0  ...      0.0      0.023873      0.500000\n",
            "75%        0.0      0.0      0.0  ...      0.0      0.023873      1.000000\n",
            "max        0.0      0.0      0.0  ...      0.0      1.000000      1.000000\n",
            "\n",
            "[8 rows x 153 columns]\n",
            "155\n",
            "0    48522\n",
            "1    48522\n",
            "dtype: int64\n",
            "(97044, 152) (97044,)\n",
            "(40158, 152) (40158,)\n",
            "(48522, 153) (48522, 153)\n",
            "Normal duplicates 484\n",
            "Attack duplicates 63\n",
            "Total duplicates 547\n",
            "Fraction duplicated 0.005636618441119492\n",
            "(97044, 153) Traning Data Shape Before drop\n",
            "(97044, 79) Traning Data Shape After drop\n",
            "\n",
            "\n",
            "Epoch 1/8\n",
            "15527/15527 [==============================] - 29s 2ms/step - loss: 0.0404 - val_loss: 0.0278\n",
            "Epoch 2/8\n",
            "15527/15527 [==============================] - 28s 2ms/step - loss: 0.0277 - val_loss: 0.0275\n",
            "Epoch 3/8\n",
            "15527/15527 [==============================] - 28s 2ms/step - loss: 0.0274 - val_loss: 0.0276\n",
            "Epoch 4/8\n",
            "15527/15527 [==============================] - 28s 2ms/step - loss: 0.0273 - val_loss: 0.0273\n",
            "Epoch 5/8\n",
            "15527/15527 [==============================] - 28s 2ms/step - loss: 0.0272 - val_loss: 0.0271\n",
            "Epoch 6/8\n",
            "15527/15527 [==============================] - 28s 2ms/step - loss: 0.0272 - val_loss: 0.0272\n",
            "Epoch 7/8\n",
            "15527/15527 [==============================] - 28s 2ms/step - loss: 0.0272 - val_loss: 0.0273\n",
            "Epoch 8/8\n",
            "15527/15527 [==============================] - 28s 2ms/step - loss: 0.0272 - val_loss: 0.0271\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 152)]             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               15300     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 75)                7575      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 50)                3800      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 20)                1020      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 100)               2100      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 152)               15352     \n",
            "=================================================================\n",
            "Total params: 45,147\n",
            "Trainable params: 45,147\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "autodencoder lr:  0.9939718687206965 \n",
            "base data lr:  0.803202350714677\n",
            "<class 'pandas.core.frame.DataFrame'> <class 'numpy.ndarray'> <class 'pandas.core.series.Series'>\n",
            "(97044, 173)\n",
            "   0    1    2         3         4    ...       168       169  170       171  172\n",
            "0    0    0    0  0.000066  0.000066  ...  0.000000  0.000000  0.0  0.000000    0\n",
            "1    0    0    0  0.000014  0.000014  ...  5.498396  0.000000  0.0  0.000000    0\n",
            "2    0    0    0  0.035528  0.035528  ...  3.619769  0.000000  0.0  1.327758    0\n",
            "3    0    0    0  0.005128  0.005128  ...  0.000000  1.357374  0.0  5.501617    0\n",
            "4    0    0    0  0.035116  0.035116  ...  3.676221  0.000000  0.0  1.289712    0\n",
            "\n",
            "[5 rows x 173 columns]\n",
            "(40158, 173)\n",
            "   0    1    2         3         4    ...       168       169  170       171  172\n",
            "0    0    0    0  0.002547  0.002547  ...  0.000000  3.739341  0.0  2.976261    0\n",
            "1    0    0    0  0.003296  0.003296  ...  0.000000  4.081227  0.0  2.992247    0\n",
            "2    0    0    0  0.003285  0.003285  ...  0.000000  4.401875  0.0  2.780365    0\n",
            "3    0    0    0  0.005942  0.005942  ...  5.416824  0.000000  0.0  2.760390    0\n",
            "4    0    0    0  0.001519  0.001519  ...  0.000000  5.606446  0.0  2.435269    0\n",
            "\n",
            "[5 rows x 173 columns]\n",
            "(97044, 172)\n",
            "(97044,)\n",
            "(40158, 172)\n",
            "(40158,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Selected Features indices: [array([  5,   6, 104])]\n",
            "Feature Ranking: [170 169 168  16  13   1   1  97  96  95  94  40  33  32  50  18 106  15\n",
            " 114 105 125 126 112  59 132 110  14 121 120 117 142 116 104 139 141  21\n",
            " 124 108 113 115  11 150 152 153  62  53 159  48  54   6 143 146 154 156\n",
            " 160 162 164 165   5  22 100  67 103  34  29   2 128  36  55  17   7 123\n",
            "  63  43  25  51  61  74 137  45  82  83  98  81 129  88  58  10 151 149\n",
            "  47  24 155 135  87   3 127  99 131 133 134  70  85  66   1  75  23  92\n",
            "   4  35  90 102 157 158  84  31  78  26  64  42  73 118  80  93  77  89\n",
            "  91  27 140 147  86 161 163 144 167  65  52   8  76  12  79  69  71  72\n",
            " 145 136 109 148 111 107 138  60 119  19  49  20 166  68  39 122 101  30\n",
            "  38  44  37  46   9  57  41  56 130  28]\n",
            "end RFE extraction\n",
            "\n",
            "Most important features: [64, 153, 47, 68, 65, 161, 48, 158, 44, 35, 79, 151, 70, 63, 167, 168, 5, 6, 164, 157, 166, 137, 171, 165, 169, 61, 154, 67, 155, 139, 142, 76, 95, 143, 105, 87, 72, 77, 117, 74, 119, 58, 107, 91, 115, 104, 162, 123, 125, 75, 127, 126, 140, 90, 124, 138, 118, 116, 163, 73, 3, 4, 106, 69, 109, 101, 108, 122, 120, 103, 45, 141, 23, 11, 59, 13, 12, 49, 15, 83, 26, 81, 40, 17, 102, 114, 110, 85, 80, 94, 135, 86, 130, 0, 1, 2, 7, 8, 9, 10, 14, 16, 18, 19, 20, 21, 22, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 43, 46, 50, 51, 52, 53, 54, 55, 56, 57, 60, 62, 66, 71, 78, 82, 84, 88, 89, 92, 93, 96, 97, 98, 99, 100, 111, 112, 113, 121, 128, 129, 131, 132, 133, 134, 136, 144, 145, 146, 147, 148, 149, 150, 152, 156, 159, 160, 170]\n",
            "[5, 6, 104]\n",
            "[64, 153, 47, 68]\n",
            "<class 'list'>\n",
            "[5, 6, 47, 64, 68, 104, 153]\n",
            "(97044, 7) (97044,)\n",
            "(40158, 7)\n",
            "(97044, 7) (97044,)\n",
            "(40158, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-TJFwNWNThw"
      },
      "source": [
        "#Load libraries and data\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "import pandas as pd\n",
        "from pandas import set_option\n",
        "from numpy import unique\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from __future__ import print_function\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.regularizers import l1\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')\n",
        "set_seed(1)\n",
        "# load csv using pandas\n",
        "filename = 'test_imperson_without4n7_balanced_data.csv'\n",
        "testdata = read_csv(filename)\n",
        "filename = 'train_imperson_without4n7_balanced_data.csv'\n",
        "traindata = read_csv(filename)\n",
        "\n",
        "\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "#investigate data\n",
        "print(\"training data shape\",traindata.shape)\n",
        "print(\"testing data shape\",testdata.shape)\n",
        "description = traindata.describe()\n",
        "print(description)\n",
        "class_counts = traindata.groupby('155').size()\n",
        "print(class_counts)\n",
        "traindata.head()\n",
        "\n",
        "#train_set, test_set = train_test_split(traindata, test_size=testdata, random_state=42)\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "x_train = traindata[columns]\n",
        "y_train = traindata[target]\n",
        "\n",
        "x_test = testdata[columns]\n",
        "y_test = testdata[target]\n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)\n",
        "good = traindata[y_train == 1]\n",
        "bad = traindata[y_train == 0]\n",
        "print(good.shape,bad.shape)\n",
        "\n",
        "normal_duplicates = sum( good.duplicated() )\n",
        "attack_duplicates = sum( bad.duplicated() )\n",
        "total_duplicates = normal_duplicates + attack_duplicates\n",
        "\n",
        "print( 'Normal duplicates', normal_duplicates )\n",
        "print( 'Attack duplicates', attack_duplicates )\n",
        "print( 'Total duplicates', total_duplicates )\n",
        "print( 'Fraction duplicated', total_duplicates / len(traindata) )\n",
        "\n",
        "df_nUnique = traindata.loc[:, traindata.nunique() == 1]\n",
        "df_nUnique.head()\n",
        "\n",
        "def Drop_Redun_Columns(df):\n",
        "  print (df.shape , 'Traning Data Shape Before drop')\n",
        "  dfdrop = df.loc[:, df.nunique() != 1]\n",
        "  print (dfdrop.shape , 'Traning Data Shape After drop')\n",
        "  return dfdrop\n",
        "\n",
        "# originally deleted non unique columns but decided to keep. Left code in to see how many are non-unique and how these correlate\n",
        "unique_training = Drop_Redun_Columns(traindata)\n",
        "print (\"\\n\")\n",
        "\n",
        "%matplotlib inline\n",
        "corrl = unique_training.corr()\n",
        "plt.imshow(corrl) ;\n",
        "\n",
        "#stacked autoencoder sparse. used by demise and detered\n",
        "# split into train test sets\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "start_time = time.time()\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)\n",
        "\n",
        "input_size = 152\n",
        "hidden_size = 100\n",
        "hidden_size_2 = 60\n",
        "hidden_size_3 = 30\n",
        "code_size = 10\n",
        "batch_size = 5\n",
        "input_data = Input(shape=(input_size,))\n",
        "encoder_1 = Dense(hidden_size, activation='relu')(input_data)\n",
        "encoder_2 = Dense(hidden_size_2, activation='relu')(encoder_1)\n",
        "encoder_3 = Dense(hidden_size_3, activation='relu')(encoder_2)\n",
        "code = Dense(code_size, activation='relu')(encoder_3)\n",
        "decoder_1 = Dense(hidden_size, activation='relu')(code)\n",
        "output_data = Dense(input_size, activation='sigmoid')(decoder_1)\n",
        "autoencoder = Model(input_data, output_data) # autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder_train = autoencoder.fit(X_train, X_train, epochs=8,validation_data=(X_test, X_test),batch_size=batch_size)\n",
        "autoencoder.summary()\n",
        "encoder = Model(input_data, code) # encoder to create new features\n",
        "\n",
        "# encode the train data\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "# encode the test data\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "# define the model\n",
        "model = LogisticRegression() #use basic logisitic regression to test the autoencoder results\n",
        "# fit the model on the encoded training set and regular training set\n",
        "model.fit(X_train_encode, Y_train)\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_encode)\n",
        "# calculate classification accuracy\n",
        "acc = accuracy_score(Y_test, yhat)\n",
        "# now test with original dataset\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "yhat2 = model.predict(x_test)\n",
        "acc2 = accuracy_score(y_test, yhat2)\n",
        "print(\"autodencoder lr: \", acc, \"\\nbase data lr: \",acc2)\n",
        "\n",
        "# trained the autoencoder and checked the accuracy. Now to encode whole dataset.\n",
        "# encode the train data\n",
        "x_train_en = encoder.predict(x_train)\n",
        "# encode the test data\n",
        "x_test_en = encoder.predict(x_test)\n",
        "\n",
        "\n",
        "# sorting out testing/trainng sets\n",
        "print(type(x_train),type(x_train_en),type(y_train))\n",
        "x_train_en = pd.DataFrame(x_train_en)\n",
        "traindata = pd.concat([x_train,x_train_en,y_train],axis=1,ignore_index=True)\n",
        "\n",
        "x_test_en = pd.DataFrame(x_test_en)\n",
        "testdata = pd.concat([x_test,x_test_en,y_test],axis=1,ignore_index=True)\n",
        "\n",
        "print(traindata.shape)\n",
        "print(traindata.head())\n",
        "print(testdata.shape)\n",
        "print(testdata.head())\n",
        "\n",
        "#split into x and y for feature selection\n",
        "x_train = traindata[traindata.columns[0:(input_size + code_size)]]\n",
        "y_train = traindata[traindata.columns[-1]]\n",
        "\n",
        "x_test = testdata[testdata.columns[0:(input_size + code_size)]]\n",
        "y_test = testdata[testdata.columns[-1]]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "%matplotlib inline\n",
        "# plot losses\n",
        "plt.close('all')\n",
        "plt.figure()\n",
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(1,9)\n",
        "plt.plot(epochs,loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs,val_loss, 'g-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# feature selection\n",
        "X = x_train\n",
        "Y = y_train\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "# feature extraction with RFE 3 features\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "model = LogisticRegression(solver='liblinear',random_state=42)\n",
        "# feature extraction\n",
        "rfe = RFE(model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "rfebestindices10 = [fit.get_support(indices=True)]\n",
        "print(\"Selected Features indices: %s\" % rfebestindices10)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
        "print(\"end RFE extraction\\n\")\n",
        "\n",
        "# feature extraction with extra trees classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# feature extraction\n",
        "extratrees = ExtraTreesClassifier(max_features=4,random_state=42)\n",
        "fit = extratrees.fit(X, Y)\n",
        "result = fit.feature_importances_\n",
        "\n",
        "important_features_dict = {}\n",
        "for x,i in enumerate(result):\n",
        "    important_features_dict[x]=i\n",
        "\n",
        "\n",
        "important_features_list = sorted(important_features_dict,\n",
        "                                 key=important_features_dict.get,\n",
        "                                 reverse=True)\n",
        "\n",
        "print( 'Most important features: %s' %important_features_list)\n",
        "\n",
        "\n",
        "rfebestindices10_unpacked = list(rfebestindices10[0])\n",
        "print(rfebestindices10_unpacked)\n",
        "print(important_features_list[:4])\n",
        "print(type(rfebestindices10_unpacked))\n",
        "features = set(rfebestindices10_unpacked + important_features_list[:4])\n",
        "features = list(features) \n",
        "features = sorted(features)\n",
        "print(features)\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, hamming_loss \n",
        "from tensorflow.random import set_seed\n",
        "from sklearn.metrics import plot_confusion_matrix,roc_curve, jaccard_score,average_precision_score,balanced_accuracy_score\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "set_seed(1)\n",
        "#selected model\n",
        "\n",
        "#building ML algorithms\n",
        "set_seed(1)\n",
        "#model creation\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "x_test = testdata.iloc[:,features]\n",
        "\n",
        "# evaluate model\n",
        "model = SVC(kernel='linear', gamma=0.1, random_state = 42,probability=True)\n",
        "kfold = KFold(n_splits=10, random_state=42,shuffle=True)\n",
        "cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
        "msg = \"Linear SVM: %f (%f)\" % (cv_results.mean(), cv_results.std())\n",
        "print(msg)\n",
        "\n",
        "%matplotlib inline\n",
        "#evaluating model and analyzing results\n",
        "set_seed(1)\n",
        "#selected model is classifier with logistic regression RFE + extra trees feature selection\n",
        "model = model.fit(X,Y)\n",
        "predictions = model.predict(x_test)\n",
        "#plot ROC AUC\n",
        "# predict probabilities\n",
        "probs = model.predict_proba(x_test)\n",
        "# keep probabilities for the attack outcome only\n",
        "probs = probs[:,1]\n",
        "# calculate AUC\n",
        "auc = roc_auc_score(y_test, probs)\n",
        "print('AUC: %.3f' % auc)\n",
        "# calculate roc curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "# plot no skill\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, predictions),classification_report(y_test, predictions))\n",
        "\n",
        "def sk_metrix(real_values,pred_values):\n",
        "   Accuracy = round( accuracy_score(real_values,pred_values) ,4)\n",
        "   \n",
        "   Precision  = round(precision_score(real_values,pred_values),4 )\n",
        "   \n",
        "   Recall     = round(recall_score(real_values,pred_values),4 )\n",
        "   F1         = round (f1_score(real_values,pred_values),4)\n",
        "   MCC        = round (matthews_corrcoef(real_values,pred_values)  ,4)\n",
        "   Hamming    = round (hamming_loss(real_values,pred_values) ,4)\n",
        "   Jaccard    = round (jaccard_score(real_values,pred_values) ,4)\n",
        "   Prec_Avg   = round (average_precision_score(real_values,pred_values) ,4)\n",
        "   Accu_Avg   = round (balanced_accuracy_score(real_values,pred_values) ,4)\n",
        "   mat_met = pd.DataFrame({\n",
        "'Metric': ['Accuracy','Precision','Recall','F1','MCC','Hamming','Jaccard'],\n",
        "'Value': [Accuracy,Precision,Recall,F1,MCC,Hamming,Jaccard]})\n",
        "   return (mat_met)\n",
        "\n",
        "sk_met = sk_metrix(y_test, predictions)\n",
        "print(sk_met)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a5IgCoQNgyM"
      },
      "source": [
        "#Load libraries and data\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "import pandas as pd\n",
        "from pandas import set_option\n",
        "from numpy import unique\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from __future__ import print_function\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.regularizers import l1\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')\n",
        "set_seed(1)\n",
        "# load csv using pandas\n",
        "filename = 'test_imperson_without4n7_balanced_data.csv'\n",
        "testdata = read_csv(filename)\n",
        "filename = 'train_imperson_without4n7_balanced_data.csv'\n",
        "traindata = read_csv(filename)\n",
        "\n",
        "\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "#investigate data\n",
        "print(\"training data shape\",traindata.shape)\n",
        "print(\"testing data shape\",testdata.shape)\n",
        "description = traindata.describe()\n",
        "print(description)\n",
        "class_counts = traindata.groupby('155').size()\n",
        "print(class_counts)\n",
        "traindata.head()\n",
        "\n",
        "#train_set, test_set = train_test_split(traindata, test_size=testdata, random_state=42)\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "x_train = traindata[columns]\n",
        "y_train = traindata[target]\n",
        "\n",
        "x_test = testdata[columns]\n",
        "y_test = testdata[target]\n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)\n",
        "good = traindata[y_train == 1]\n",
        "bad = traindata[y_train == 0]\n",
        "print(good.shape,bad.shape)\n",
        "\n",
        "normal_duplicates = sum( good.duplicated() )\n",
        "attack_duplicates = sum( bad.duplicated() )\n",
        "total_duplicates = normal_duplicates + attack_duplicates\n",
        "\n",
        "print( 'Normal duplicates', normal_duplicates )\n",
        "print( 'Attack duplicates', attack_duplicates )\n",
        "print( 'Total duplicates', total_duplicates )\n",
        "print( 'Fraction duplicated', total_duplicates / len(traindata) )\n",
        "\n",
        "df_nUnique = traindata.loc[:, traindata.nunique() == 1]\n",
        "df_nUnique.head()\n",
        "\n",
        "def Drop_Redun_Columns(df):\n",
        "  print (df.shape , 'Traning Data Shape Before drop')\n",
        "  dfdrop = df.loc[:, df.nunique() != 1]\n",
        "  print (dfdrop.shape , 'Traning Data Shape After drop')\n",
        "  return dfdrop\n",
        "\n",
        "# originally deleted non unique columns but decided to keep. Left code in to see how many are non-unique and how these correlate\n",
        "unique_training = Drop_Redun_Columns(traindata)\n",
        "print (\"\\n\")\n",
        "\n",
        "%matplotlib inline\n",
        "corrl = unique_training.corr()\n",
        "plt.imshow(corrl) ;\n",
        "\n",
        "#stacked autoencoder sparse. used by demise and detered\n",
        "# split into train test sets\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "start_time = time.time()\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)\n",
        "\n",
        "input_size = 152\n",
        "hidden_size = 100\n",
        "hidden_size_2 = 80\n",
        "hidden_size_3 = 60\n",
        "code_size = 40\n",
        "batch_size = 5\n",
        "input_data = Input(shape=(input_size,))\n",
        "encoder_1 = Dense(hidden_size, activation='relu')(input_data)\n",
        "encoder_2 = Dense(hidden_size_2, activation='relu')(encoder_1)\n",
        "encoder_3 = Dense(hidden_size_3, activation='relu')(encoder_2)\n",
        "code = Dense(code_size, activation='relu')(encoder_3)\n",
        "decoder_1 = Dense(hidden_size, activation='relu')(code)\n",
        "output_data = Dense(input_size, activation='sigmoid')(decoder_1)\n",
        "autoencoder = Model(input_data, output_data) # autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder_train = autoencoder.fit(X_train, X_train, epochs=8,validation_data=(X_test, X_test),batch_size=batch_size)\n",
        "autoencoder.summary()\n",
        "encoder = Model(input_data, code) # encoder to create new features\n",
        "\n",
        "# encode the train data\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "# encode the test data\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "# define the model\n",
        "model = LogisticRegression() #use basic logisitic regression to test the autoencoder results\n",
        "# fit the model on the encoded training set and regular training set\n",
        "model.fit(X_train_encode, Y_train)\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_encode)\n",
        "# calculate classification accuracy\n",
        "acc = accuracy_score(Y_test, yhat)\n",
        "# now test with original dataset\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "yhat2 = model.predict(x_test)\n",
        "acc2 = accuracy_score(y_test, yhat2)\n",
        "print(\"autodencoder lr: \", acc, \"\\nbase data lr: \",acc2)\n",
        "\n",
        "# trained the autoencoder and checked the accuracy. Now to encode whole dataset.\n",
        "# encode the train data\n",
        "x_train_en = encoder.predict(x_train)\n",
        "# encode the test data\n",
        "x_test_en = encoder.predict(x_test)\n",
        "\n",
        "\n",
        "# sorting out testing/trainng sets\n",
        "print(type(x_train),type(x_train_en),type(y_train))\n",
        "x_train_en = pd.DataFrame(x_train_en)\n",
        "traindata = pd.concat([x_train,x_train_en,y_train],axis=1,ignore_index=True)\n",
        "\n",
        "x_test_en = pd.DataFrame(x_test_en)\n",
        "testdata = pd.concat([x_test,x_test_en,y_test],axis=1,ignore_index=True)\n",
        "\n",
        "print(traindata.shape)\n",
        "print(traindata.head())\n",
        "print(testdata.shape)\n",
        "print(testdata.head())\n",
        "\n",
        "#split into x and y for feature selection\n",
        "x_train = traindata[traindata.columns[0:(input_size + code_size)]]\n",
        "y_train = traindata[traindata.columns[-1]]\n",
        "\n",
        "x_test = testdata[testdata.columns[0:(input_size + code_size)]]\n",
        "y_test = testdata[testdata.columns[-1]]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "%matplotlib inline\n",
        "# plot losses\n",
        "plt.close('all')\n",
        "plt.figure()\n",
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(1,9)\n",
        "plt.plot(epochs,loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs,val_loss, 'g-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# feature selection\n",
        "X = x_train\n",
        "Y = y_train\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "# feature extraction with RFE 3 features\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "model = LogisticRegression(solver='liblinear',random_state=42)\n",
        "# feature extraction\n",
        "rfe = RFE(model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "rfebestindices10 = [fit.get_support(indices=True)]\n",
        "print(\"Selected Features indices: %s\" % rfebestindices10)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
        "print(\"end RFE extraction\\n\")\n",
        "\n",
        "# feature extraction with extra trees classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# feature extraction\n",
        "extratrees = ExtraTreesClassifier(max_features=4,random_state=42)\n",
        "fit = extratrees.fit(X, Y)\n",
        "result = fit.feature_importances_\n",
        "\n",
        "important_features_dict = {}\n",
        "for x,i in enumerate(result):\n",
        "    important_features_dict[x]=i\n",
        "\n",
        "\n",
        "important_features_list = sorted(important_features_dict,\n",
        "                                 key=important_features_dict.get,\n",
        "                                 reverse=True)\n",
        "\n",
        "print( 'Most important features: %s' %important_features_list)\n",
        "\n",
        "\n",
        "rfebestindices10_unpacked = list(rfebestindices10[0])\n",
        "print(rfebestindices10_unpacked)\n",
        "print(important_features_list[:4])\n",
        "print(type(rfebestindices10_unpacked))\n",
        "features = set(rfebestindices10_unpacked + important_features_list[:4])\n",
        "features = list(features) \n",
        "features = sorted(features)\n",
        "print(features)\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, hamming_loss \n",
        "from tensorflow.random import set_seed\n",
        "from sklearn.metrics import plot_confusion_matrix,roc_curve, jaccard_score,average_precision_score,balanced_accuracy_score\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "set_seed(1)\n",
        "#selected model\n",
        "\n",
        "#building ML algorithms\n",
        "set_seed(1)\n",
        "#model creation\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "x_test = testdata.iloc[:,features]\n",
        "\n",
        "# evaluate model\n",
        "model = SVC(kernel='linear', gamma=0.1, random_state = 42,probability=True)\n",
        "kfold = KFold(n_splits=10, random_state=42,shuffle=True)\n",
        "cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
        "msg = \"Linear SVM: %f (%f)\" % (cv_results.mean(), cv_results.std())\n",
        "print(msg)\n",
        "\n",
        "%matplotlib inline\n",
        "#evaluating model and analyzing results\n",
        "set_seed(1)\n",
        "#selected model is classifier with logistic regression RFE + extra trees feature selection\n",
        "model = model.fit(X,Y)\n",
        "predictions = model.predict(x_test)\n",
        "#plot ROC AUC\n",
        "# predict probabilities\n",
        "probs = model.predict_proba(x_test)\n",
        "# keep probabilities for the attack outcome only\n",
        "probs = probs[:,1]\n",
        "# calculate AUC\n",
        "auc = roc_auc_score(y_test, probs)\n",
        "print('AUC: %.3f' % auc)\n",
        "# calculate roc curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "# plot no skill\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, predictions),classification_report(y_test, predictions))\n",
        "\n",
        "def sk_metrix(real_values,pred_values):\n",
        "   Accuracy = round( accuracy_score(real_values,pred_values) ,4)\n",
        "   \n",
        "   Precision  = round(precision_score(real_values,pred_values),4 )\n",
        "   \n",
        "   Recall     = round(recall_score(real_values,pred_values),4 )\n",
        "   F1         = round (f1_score(real_values,pred_values),4)\n",
        "   MCC        = round (matthews_corrcoef(real_values,pred_values)  ,4)\n",
        "   Hamming    = round (hamming_loss(real_values,pred_values) ,4)\n",
        "   Jaccard    = round (jaccard_score(real_values,pred_values) ,4)\n",
        "   Prec_Avg   = round (average_precision_score(real_values,pred_values) ,4)\n",
        "   Accu_Avg   = round (balanced_accuracy_score(real_values,pred_values) ,4)\n",
        "   mat_met = pd.DataFrame({\n",
        "'Metric': ['Accuracy','Precision','Recall','F1','MCC','Hamming','Jaccard'],\n",
        "'Value': [Accuracy,Precision,Recall,F1,MCC,Hamming,Jaccard]})\n",
        "   return (mat_met)\n",
        "\n",
        "sk_met = sk_metrix(y_test, predictions)\n",
        "print(sk_met)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4FlEtT8N2op"
      },
      "source": [
        "#Load libraries and data\n",
        "from pandas import read_csv\n",
        "from numpy import set_printoptions\n",
        "import pandas as pd\n",
        "from pandas import set_option\n",
        "from numpy import unique\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from __future__ import print_function\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.regularizers import l1\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')\n",
        "set_seed(1)\n",
        "# load csv using pandas\n",
        "filename = 'test_imperson_without4n7_balanced_data.csv'\n",
        "testdata = read_csv(filename)\n",
        "filename = 'train_imperson_without4n7_balanced_data.csv'\n",
        "traindata = read_csv(filename)\n",
        "\n",
        "\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "#investigate data\n",
        "print(\"training data shape\",traindata.shape)\n",
        "print(\"testing data shape\",testdata.shape)\n",
        "description = traindata.describe()\n",
        "print(description)\n",
        "class_counts = traindata.groupby('155').size()\n",
        "print(class_counts)\n",
        "traindata.head()\n",
        "\n",
        "#train_set, test_set = train_test_split(traindata, test_size=testdata, random_state=42)\n",
        "columns = [c for c in traindata.columns if c not in ['155']]\n",
        "target = '155'\n",
        "\n",
        "x_train = traindata[columns]\n",
        "y_train = traindata[target]\n",
        "\n",
        "x_test = testdata[columns]\n",
        "y_test = testdata[target]\n",
        "\n",
        "print(x_train.shape,y_train.shape)\n",
        "print(x_test.shape,y_test.shape)\n",
        "good = traindata[y_train == 1]\n",
        "bad = traindata[y_train == 0]\n",
        "print(good.shape,bad.shape)\n",
        "\n",
        "normal_duplicates = sum( good.duplicated() )\n",
        "attack_duplicates = sum( bad.duplicated() )\n",
        "total_duplicates = normal_duplicates + attack_duplicates\n",
        "\n",
        "print( 'Normal duplicates', normal_duplicates )\n",
        "print( 'Attack duplicates', attack_duplicates )\n",
        "print( 'Total duplicates', total_duplicates )\n",
        "print( 'Fraction duplicated', total_duplicates / len(traindata) )\n",
        "\n",
        "df_nUnique = traindata.loc[:, traindata.nunique() == 1]\n",
        "df_nUnique.head()\n",
        "\n",
        "def Drop_Redun_Columns(df):\n",
        "  print (df.shape , 'Traning Data Shape Before drop')\n",
        "  dfdrop = df.loc[:, df.nunique() != 1]\n",
        "  print (dfdrop.shape , 'Traning Data Shape After drop')\n",
        "  return dfdrop\n",
        "\n",
        "# originally deleted non unique columns but decided to keep. Left code in to see how many are non-unique and how these correlate\n",
        "unique_training = Drop_Redun_Columns(traindata)\n",
        "print (\"\\n\")\n",
        "\n",
        "%matplotlib inline\n",
        "corrl = unique_training.corr()\n",
        "plt.imshow(corrl) ;\n",
        "\n",
        "#stacked autoencoder sparse. used by demise and detered\n",
        "# split into train test sets\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "\n",
        "start_time = time.time()\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)\n",
        "\n",
        "input_size = 152\n",
        "hidden_size = 100\n",
        "hidden_size_2 = 80\n",
        "hidden_size_3 = 60\n",
        "code_size = 10\n",
        "batch_size = 5\n",
        "input_data = Input(shape=(input_size,))\n",
        "encoder_1 = Dense(hidden_size, activation='relu')(input_data)\n",
        "encoder_2 = Dense(hidden_size_2, activation='relu')(encoder_1)\n",
        "encoder_3 = Dense(hidden_size_3, activation='relu')(encoder_2)\n",
        "code = Dense(code_size, activation='relu')(encoder_3)\n",
        "decoder_1 = Dense(hidden_size, activation='relu')(code)\n",
        "output_data = Dense(input_size, activation='sigmoid')(decoder_1)\n",
        "autoencoder = Model(input_data, output_data) # autoencoder model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder_train = autoencoder.fit(X_train, X_train, epochs=8,validation_data=(X_test, X_test),batch_size=batch_size)\n",
        "autoencoder.summary()\n",
        "encoder = Model(input_data, code) # encoder to create new features\n",
        "\n",
        "# encode the train data\n",
        "X_train_encode = encoder.predict(X_train)\n",
        "# encode the test data\n",
        "X_test_encode = encoder.predict(X_test)\n",
        "# define the model\n",
        "model = LogisticRegression() #use basic logisitic regression to test the autoencoder results\n",
        "# fit the model on the encoded training set and regular training set\n",
        "model.fit(X_train_encode, Y_train)\n",
        "# make predictions on the test set\n",
        "yhat = model.predict(X_test_encode)\n",
        "# calculate classification accuracy\n",
        "acc = accuracy_score(Y_test, yhat)\n",
        "# now test with original dataset\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "yhat2 = model.predict(x_test)\n",
        "acc2 = accuracy_score(y_test, yhat2)\n",
        "print(\"autodencoder lr: \", acc, \"\\nbase data lr: \",acc2)\n",
        "\n",
        "# trained the autoencoder and checked the accuracy. Now to encode whole dataset.\n",
        "# encode the train data\n",
        "x_train_en = encoder.predict(x_train)\n",
        "# encode the test data\n",
        "x_test_en = encoder.predict(x_test)\n",
        "\n",
        "\n",
        "# sorting out testing/trainng sets\n",
        "print(type(x_train),type(x_train_en),type(y_train))\n",
        "x_train_en = pd.DataFrame(x_train_en)\n",
        "traindata = pd.concat([x_train,x_train_en,y_train],axis=1,ignore_index=True)\n",
        "\n",
        "x_test_en = pd.DataFrame(x_test_en)\n",
        "testdata = pd.concat([x_test,x_test_en,y_test],axis=1,ignore_index=True)\n",
        "\n",
        "print(traindata.shape)\n",
        "print(traindata.head())\n",
        "print(testdata.shape)\n",
        "print(testdata.head())\n",
        "\n",
        "#split into x and y for feature selection\n",
        "x_train = traindata[traindata.columns[0:(input_size + code_size)]]\n",
        "y_train = traindata[traindata.columns[-1]]\n",
        "\n",
        "x_test = testdata[testdata.columns[0:(input_size + code_size)]]\n",
        "y_test = testdata[testdata.columns[-1]]\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.random import set_seed\n",
        "\n",
        "set_seed(1)\n",
        "%matplotlib inline\n",
        "# plot losses\n",
        "plt.close('all')\n",
        "plt.figure()\n",
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(1,9)\n",
        "plt.plot(epochs,loss, 'b-', label='Training loss')\n",
        "plt.plot(epochs,val_loss, 'g-', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# feature selection\n",
        "X = x_train\n",
        "Y = y_train\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "# feature extraction with RFE 3 features\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "model = LogisticRegression(solver='liblinear',random_state=42)\n",
        "# feature extraction\n",
        "rfe = RFE(model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "rfebestindices10 = [fit.get_support(indices=True)]\n",
        "print(\"Selected Features indices: %s\" % rfebestindices10)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
        "print(\"end RFE extraction\\n\")\n",
        "\n",
        "# feature extraction with extra trees classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# feature extraction\n",
        "extratrees = ExtraTreesClassifier(max_features=3,random_state=42)\n",
        "fit = extratrees.fit(X, Y)\n",
        "result = fit.feature_importances_\n",
        "\n",
        "important_features_dict = {}\n",
        "for x,i in enumerate(result):\n",
        "    important_features_dict[x]=i\n",
        "\n",
        "\n",
        "important_features_list = sorted(important_features_dict,\n",
        "                                 key=important_features_dict.get,\n",
        "                                 reverse=True)\n",
        "\n",
        "print( 'Most important features: %s' %important_features_list)\n",
        "\n",
        "\n",
        "rfebestindices10_unpacked = list(rfebestindices10[0])\n",
        "print(rfebestindices10_unpacked)\n",
        "print(important_features_list[:4])\n",
        "print(type(rfebestindices10_unpacked))\n",
        "features = set(rfebestindices10_unpacked + important_features_list[:4])\n",
        "features = list(features) \n",
        "features = sorted(features)\n",
        "print(features)\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, hamming_loss \n",
        "from tensorflow.random import set_seed\n",
        "from sklearn.metrics import plot_confusion_matrix,roc_curve, jaccard_score,average_precision_score,balanced_accuracy_score\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "set_seed(1)\n",
        "#selected model\n",
        "\n",
        "#building ML algorithms\n",
        "set_seed(1)\n",
        "#model creation\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "x_test = testdata.iloc[:,features]\n",
        "\n",
        "# evaluate model\n",
        "model = SVC(kernel='linear', gamma=0.1, random_state = 42,probability=True)\n",
        "kfold = KFold(n_splits=10, random_state=42,shuffle=True)\n",
        "cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
        "msg = \"Linear SVM: %f (%f)\" % (cv_results.mean(), cv_results.std())\n",
        "print(msg)\n",
        "\n",
        "%matplotlib inline\n",
        "#evaluating model and analyzing results\n",
        "set_seed(1)\n",
        "#selected model is classifier with logistic regression RFE + extra trees feature selection\n",
        "model = model.fit(X,Y)\n",
        "predictions = model.predict(x_test)\n",
        "#plot ROC AUC\n",
        "# predict probabilities\n",
        "probs = model.predict_proba(x_test)\n",
        "# keep probabilities for the attack outcome only\n",
        "probs = probs[:,1]\n",
        "# calculate AUC\n",
        "auc = roc_auc_score(y_test, probs)\n",
        "print('AUC: %.3f' % auc)\n",
        "# calculate roc curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "# plot no skill\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, predictions),classification_report(y_test, predictions))\n",
        "\n",
        "def sk_metrix(real_values,pred_values):\n",
        "   Accuracy = round( accuracy_score(real_values,pred_values) ,4)\n",
        "   \n",
        "   Precision  = round(precision_score(real_values,pred_values),4 )\n",
        "   \n",
        "   Recall     = round(recall_score(real_values,pred_values),4 )\n",
        "   F1         = round (f1_score(real_values,pred_values),4)\n",
        "   MCC        = round (matthews_corrcoef(real_values,pred_values)  ,4)\n",
        "   Hamming    = round (hamming_loss(real_values,pred_values) ,4)\n",
        "   Jaccard    = round (jaccard_score(real_values,pred_values) ,4)\n",
        "   Prec_Avg   = round (average_precision_score(real_values,pred_values) ,4)\n",
        "   Accu_Avg   = round (balanced_accuracy_score(real_values,pred_values) ,4)\n",
        "   mat_met = pd.DataFrame({\n",
        "'Metric': ['Accuracy','Precision','Recall','F1','MCC','Hamming','Jaccard'],\n",
        "'Value': [Accuracy,Precision,Recall,F1,MCC,Hamming,Jaccard]})\n",
        "   return (mat_met)\n",
        "\n",
        "sk_met = sk_metrix(y_test, predictions)\n",
        "print(sk_met)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EDl21xUsjNww",
        "outputId": "4733ce53-4379-493c-a3ed-80bef1f41133"
      },
      "source": [
        "set_seed(1)\n",
        "# feature selection\n",
        "X = x_train\n",
        "Y = y_train\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "# feature extraction with RFE 4 features\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "model = LogisticRegression(solver='liblinear',random_state=42)\n",
        "# feature extraction\n",
        "rfe = RFE(model, n_features_to_select=3)\n",
        "fit = rfe.fit(X, Y)\n",
        "rfebestindices10 = [fit.get_support(indices=True)]\n",
        "print(\"Selected Features indices: %s\" % rfebestindices10)\n",
        "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
        "print(\"end RFE extraction\\n\")\n",
        "\n",
        "# feature extraction with extra trees classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "# feature extraction\n",
        "extratrees = ExtraTreesClassifier(max_features=4,random_state=42)\n",
        "fit = extratrees.fit(X, Y)\n",
        "result = fit.feature_importances_\n",
        "\n",
        "important_features_dict = {}\n",
        "for x,i in enumerate(result):\n",
        "    important_features_dict[x]=i\n",
        "\n",
        "\n",
        "important_features_list = sorted(important_features_dict,\n",
        "                                 key=important_features_dict.get,\n",
        "                                 reverse=True)\n",
        "\n",
        "print( 'Most important features: %s' %important_features_list)\n",
        "\n",
        "\n",
        "rfebestindices10_unpacked = list(rfebestindices10[0])\n",
        "print(rfebestindices10_unpacked)\n",
        "print(important_features_list[:5])\n",
        "print(type(rfebestindices10_unpacked))\n",
        "features = set(rfebestindices10_unpacked + important_features_list[:4])\n",
        "features = list(features) \n",
        "features = sorted(features)\n",
        "print(features)\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score, matthews_corrcoef, hamming_loss \n",
        "from tensorflow.random import set_seed\n",
        "from sklearn.metrics import plot_confusion_matrix,roc_curve, jaccard_score,average_precision_score,balanced_accuracy_score\n",
        "#setting X and Y to best results from above\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "print(X.shape,Y.shape)\n",
        "x_test = testdata.iloc[:,features]\n",
        "print(x_test.shape)\n",
        "\n",
        "set_seed(1)\n",
        "#selected model\n",
        "\n",
        "#building ML algorithms\n",
        "set_seed(1)\n",
        "#model creation\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "x_test = testdata.iloc[:,features]\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='liblinear',random_state = 42)))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('ADABOOST CLASSIFIER',AdaBoostClassifier()))\n",
        "models.append(('linear SVM',SVC(kernel='linear', gamma=0.1, random_state = 42)))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=42,shuffle=True)\n",
        "  cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "  clf = model\n",
        "  clf.fit(X, Y)\n",
        "  y_pred = model.predict(x_test)\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "# evaluate model\n",
        "model = SVC(kernel='linear', gamma=0.1, random_state = 42)\n",
        "kfold = KFold(n_splits=10, random_state=42,shuffle=True)\n",
        "cv_results = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
        "msg = \"Adaboost Classifier: %f (%f)\" % (cv_results.mean(), cv_results.std())\n",
        "print(msg)\n",
        "\n",
        "%matplotlib inline\n",
        "#evaluating model and analyzing results\n",
        "set_seed(1)\n",
        "#selected model is adaboost classifier with logistic regression RFE + extra trees feature selection\n",
        "model = model.fit(X,Y)\n",
        "predictions = model.predict(x_test)\n",
        "#plot ROC AUC\n",
        "# predict probabilities\n",
        "probs = model.predict_proba(x_test)\n",
        "# keep probabilities for the attack outcome only\n",
        "probs = probs[:,1]\n",
        "# calculate AUC\n",
        "auc = roc_auc_score(y_test, probs)\n",
        "print('AUC: %.3f' % auc)\n",
        "# calculate roc curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "# plot no skill\n",
        "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
        "# plot the roc curve for the model\n",
        "pyplot.plot(fpr, tpr, marker='.')\n",
        "# show the plot\n",
        "pyplot.show()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(confusion_matrix(y_test, predictions),classification_report(y_test, predictions))\n",
        "\n",
        "def sk_metrix(real_values,pred_values):\n",
        "   Accuracy = round( accuracy_score(real_values,pred_values) ,4)\n",
        "   \n",
        "   Precision  = round(precision_score(real_values,pred_values),4 )\n",
        "   \n",
        "   Recall     = round(recall_score(real_values,pred_values),4 )\n",
        "   F1         = round (f1_score(real_values,pred_values),4)\n",
        "   MCC        = round (matthews_corrcoef(real_values,pred_values)  ,4)\n",
        "   Hamming    = round (hamming_loss(real_values,pred_values) ,4)\n",
        "   Jaccard    = round (jaccard_score(real_values,pred_values) ,4)\n",
        "   Prec_Avg   = round (average_precision_score(real_values,pred_values) ,4)\n",
        "   Accu_Avg   = round (balanced_accuracy_score(real_values,pred_values) ,4)\n",
        "   mat_met = pd.DataFrame({\n",
        "'Metric': ['Accuracy','Precision','Recall','F1','MCC','Hamming','Jaccard'],\n",
        "'Value': [Accuracy,Precision,Recall,F1,MCC,Hamming,Jaccard]})\n",
        "   return (mat_met)\n",
        "\n",
        "sk_met = sk_metrix(y_test, predictions)\n",
        "print(sk_met)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected Features indices: [array([  6,  70, 104])]\n",
            "Feature Ranking: [160 159 158  19  16  13   1  90  89  88  87  52  51  50  63  48  86  59\n",
            "  97  92 104 102 106  46 122 114  45 115 117 128 120 134 136 138 140   3\n",
            " 144 146 145 143  44 127 129 125  30  61 123  39  29  40  99  96 108 130\n",
            " 147 149 152 154   6  60 142  55 113  36  22  12 111   4  53   5   1 107\n",
            "  35  34  31  21  24  62 124  10  84  80 157  82 141  85  58  17 112 116\n",
            "  18  11 135 137  81  15  94 110  98 103 121  25  70  43   1  69  27  74\n",
            "  23  14  77 148 100 109  83   2  67  26  42  28  38 132  72  78  64  75\n",
            "  76   9  91  95  79 133 131 101 105  68  66   7  65  20  71  73  57  54\n",
            " 150 118 153 155 156 151 126  41  56 139  49   8 119  32  33  93  37  47]\n",
            "end RFE extraction\n",
            "\n",
            "Most important features: [64, 47, 68, 44, 70, 65, 48, 6, 160, 79, 151, 137, 5, 155, 35, 158, 61, 63, 157, 152, 139, 142, 77, 58, 107, 161, 105, 76, 119, 143, 67, 95, 104, 115, 87, 91, 75, 123, 116, 117, 127, 125, 138, 154, 72, 140, 90, 74, 124, 126, 4, 73, 118, 3, 69, 109, 101, 106, 108, 120, 122, 103, 141, 45, 11, 15, 26, 12, 13, 40, 81, 59, 83, 17, 102, 86, 85, 23, 114, 110, 80, 135, 130, 94, 49, 0, 1, 2, 7, 8, 9, 10, 14, 16, 18, 19, 20, 21, 22, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 38, 39, 41, 42, 43, 46, 50, 51, 52, 53, 54, 55, 56, 57, 60, 62, 66, 71, 78, 82, 84, 88, 89, 92, 93, 96, 97, 98, 99, 100, 111, 112, 113, 121, 128, 129, 131, 132, 133, 134, 136, 144, 145, 146, 147, 148, 149, 150, 153, 156, 159]\n",
            "[6, 70, 104]\n",
            "[64, 47, 68, 44, 70]\n",
            "<class 'list'>\n",
            "[6, 44, 47, 64, 68, 70, 104]\n",
            "(97044, 7) (97044,)\n",
            "(40158, 7)\n",
            "(97044, 7) (97044,)\n",
            "(40158, 7)\n",
            "LR: 0.941635 (0.001385)\n",
            "[[19081   998]\n",
            " [ 1306 18773]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.95      0.94     20079\n",
            "           1       0.95      0.93      0.94     20079\n",
            "\n",
            "    accuracy                           0.94     40158\n",
            "   macro avg       0.94      0.94      0.94     40158\n",
            "weighted avg       0.94      0.94      0.94     40158\n",
            "\n",
            "LDA: 0.927219 (0.001657)\n",
            "[[17955  2124]\n",
            " [ 1465 18614]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.89      0.91     20079\n",
            "           1       0.90      0.93      0.91     20079\n",
            "\n",
            "    accuracy                           0.91     40158\n",
            "   macro avg       0.91      0.91      0.91     40158\n",
            "weighted avg       0.91      0.91      0.91     40158\n",
            "\n",
            "ADABOOST CLASSIFIER: 0.987542 (0.001471)\n",
            "[[19244   835]\n",
            " [18616  1463]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.96      0.66     20079\n",
            "           1       0.64      0.07      0.13     20079\n",
            "\n",
            "    accuracy                           0.52     40158\n",
            "   macro avg       0.57      0.52      0.40     40158\n",
            "weighted avg       0.57      0.52      0.40     40158\n",
            "\n",
            "linear SVM: 0.974032 (0.001529)\n",
            "[[18895  1184]\n",
            " [    5 20074]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97     20079\n",
            "           1       0.94      1.00      0.97     20079\n",
            "\n",
            "    accuracy                           0.97     40158\n",
            "   macro avg       0.97      0.97      0.97     40158\n",
            "weighted avg       0.97      0.97      0.97     40158\n",
            "\n",
            "Adaboost Classifier: 0.974032 (0.001529)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-6b7703772845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m#plot ROC AUC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m# predict probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;31m# keep probabilities for the attack outcome only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[1;32m    604\u001b[0m                                  \" probability=False\")\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'c_svc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nu_svc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rae8vFjupJjm",
        "outputId": "cd5cc649-9b74-48e9-b5f1-6164c3f5217b"
      },
      "source": [
        "#building ML algorithms\n",
        "set_seed(1)\n",
        "#model creation\n",
        "X = traindata.iloc[:,features]\n",
        "Y = y_train\n",
        "x_test = testdata.iloc[:,features]\n",
        "# prepare models\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(solver='liblinear',random_state = 42)))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('ADABOOST CLASSIFIER',AdaBoostClassifier()))\n",
        "models.append(('linear SVM',SVC(kernel='linear', gamma=0.1, random_state = 42)))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "results = []\n",
        "names = []\n",
        "scoring = 'accuracy'\n",
        "for name, model in models:\n",
        "  kfold = KFold(n_splits=10, random_state=42,shuffle=True)\n",
        "  cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "  print(msg)\n",
        "  clf = model\n",
        "  clf.fit(X, Y)\n",
        "  y_pred = model.predict(x_test)\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "  print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR: 0.972693 (0.001685)\n",
            "[[18253  1826]\n",
            " [   70 20009]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.91      0.95     20079\n",
            "           1       0.92      1.00      0.95     20079\n",
            "\n",
            "    accuracy                           0.95     40158\n",
            "   macro avg       0.96      0.95      0.95     40158\n",
            "weighted avg       0.96      0.95      0.95     40158\n",
            "\n",
            "LDA: 0.960265 (0.001673)\n",
            "[[17903  2176]\n",
            " [   31 20048]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.89      0.94     20079\n",
            "           1       0.90      1.00      0.95     20079\n",
            "\n",
            "    accuracy                           0.95     40158\n",
            "   macro avg       0.95      0.95      0.94     40158\n",
            "weighted avg       0.95      0.95      0.94     40158\n",
            "\n",
            "Naive Bayes: 0.956298 (0.004321)\n",
            "[[18455  1624]\n",
            " [20072     7]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.92      0.63     20079\n",
            "           1       0.00      0.00      0.00     20079\n",
            "\n",
            "    accuracy                           0.46     40158\n",
            "   macro avg       0.24      0.46      0.32     40158\n",
            "weighted avg       0.24      0.46      0.32     40158\n",
            "\n",
            "ADABOOST CLASSIFIER: 0.999608 (0.000276)\n",
            "[[18113  1966]\n",
            " [   80 19999]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95     20079\n",
            "           1       0.91      1.00      0.95     20079\n",
            "\n",
            "    accuracy                           0.95     40158\n",
            "   macro avg       0.95      0.95      0.95     40158\n",
            "weighted avg       0.95      0.95      0.95     40158\n",
            "\n",
            "linear SVM: 0.978793 (0.001746)\n",
            "[[18891  1188]\n",
            " [   85 19994]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97     20079\n",
            "           1       0.94      1.00      0.97     20079\n",
            "\n",
            "    accuracy                           0.97     40158\n",
            "   macro avg       0.97      0.97      0.97     40158\n",
            "weighted avg       0.97      0.97      0.97     40158\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84fXHuAcwX0H",
        "outputId": "4d135d7c-5ee1-4266-a834-2908fa56c690"
      },
      "source": [
        "print(features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[44, 61, 64, 104, 127, 127]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDuQONerxOUA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgIOK_qVHQOr"
      },
      "source": [
        "# **Appendix - containing extra code**\n",
        "not used for model but ran in order to find best results"
      ]
    }
  ]
}